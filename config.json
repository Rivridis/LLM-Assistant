{"path": "B:/Github/LLM-Assistant/model/neuralhermes-2.5-mistral-7b.Q5_K_M.gguf", "gpu_offload": 20, "context_length": 2048, "cpu_threads": 4, "n_batch": 512, "flash_attention": 1, "max_tokens": -1, "temperature": 0.8, "top_k": 40, "repeat_penalty": 1.1, "min_p_sampling": 0.05, "max_p_sampling": 0.05}